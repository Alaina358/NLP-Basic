{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text-Semantic-Similarity.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bd67nMRvBg36",
        "outputId": "395664b3-5548-4641-a33b-787ef3850cbd"
      },
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from itertools import product\n",
        "import numpy\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HBY96eEW2zn"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from itertools import product\n",
        "import numpy\n",
        "\n",
        "# str1 = \"Abhishek is a good boy.\"\n",
        "# str2 = \"Abhishek is not a bad boy.\"\n",
        "# str1 = \"Cat is drinking water.\"\n",
        "# str2 = \"Lions eat flesh.\"\n",
        "# str1 = \"He loves to play football.\"\n",
        "# str2 = \"Football is his favourite sport.\"\n",
        "# str1 = \"Many consider Maradona as the best player in soccer history.\"\n",
        "# str2 = \"Maradona is one of the best soccer player.\"\n",
        "\n",
        "str1 = \"Debugging is the process that results in the removal of error. It is very important part of the successful testing.\"\n",
        "str2 = \"It is a process of removing errors.\"\n",
        "\n",
        "# str1 = \"Ballmer has been vocal in the past warning that Linux is a threat to Microsoft.\"\n",
        "# str2 = \"In the memo, Ballmer reiterated the open-source threat to Microsoft.\"\n",
        "# str1 = \"The boy is fetching water from the well.\"\n",
        "# str2 = \"The lion is running in the forest.\"\n",
        "# str1 = \"A school is a place where kids go to study.\"\n",
        "# str2 = \"School is an institution for children who want to study.\"\n",
        "# str1 = \"The world knows it has lost a heroic champion of justice and freedom.\"\n",
        "# str2 = \"The earth recognizes the loss of a valiant champion of independence and justice.\"\n",
        "# str1 = \"A cemetery is a place where dead people's bodies or their ashes are buried.\"\n",
        "# str2 = \"A graveyard is an area of land ,sometimes near a church, where dead people are buried.\" \n",
        "\n",
        "##---------------Defining stopwords for English Language---------------##\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEOMjeGmW4b9"
      },
      "source": [
        "filtered_sentence1 = []\n",
        "filtered_sentence2 = []\n",
        "lemm_sentence1 = []\n",
        "lemm_sentence2 = []\n",
        "sims = []\n",
        "temp1 = []\n",
        "temp2 = []\n",
        "simi = []\n",
        "final = []\n",
        "same_sent1 = []\n",
        "same_sent2 = []"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvO0SyIDX8La"
      },
      "source": [
        "lemmatizer  =  WordNetLemmatizer()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWJm4QHIYHxx",
        "outputId": "1a06db0b-7c24-4cd3-9dda-e2f948564ea4"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLtNWVdnX_Wp"
      },
      "source": [
        "for words1 in word_tokenize(str1):\n",
        "    if words1 not in stop_words:\n",
        "        if words1.isalnum():\n",
        "            filtered_sentence1.append(words1)\n",
        "\n",
        "##---------------Lemmatizing: Root Words---------------##\n",
        "\n",
        "for i in filtered_sentence1:\n",
        "    lemm_sentence1.append(lemmatizer.lemmatize(i))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoVDV_5sYKii"
      },
      "source": [
        "for words2 in word_tokenize(str2):\n",
        "    if words2 not in stop_words:\n",
        "        if words2.isalnum():\n",
        "            filtered_sentence2.append(words2)\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12w9FnNRYNsx"
      },
      "source": [
        "for i in filtered_sentence2:\n",
        "    lemm_sentence2.append(lemmatizer.lemmatize(i))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZFlzj3OYRjY"
      },
      "source": [
        "for word1 in lemm_sentence1:\n",
        "    simi =[]\n",
        "    for word2 in lemm_sentence2:\n",
        "        sims = []\n",
        "       # print(word1)\n",
        "        #print(word2)\n",
        "        syns1 = wordnet.synsets(word1)\n",
        "        #print(syns1)\n",
        "        #print(wordFromList1[0])\n",
        "        syns2 = wordnet.synsets(word2)\n",
        "        #print(wordFromList2[0])\n",
        "        for sense1, sense2 in product(syns1, syns2):\n",
        "            d = wordnet.wup_similarity(sense1, sense2)\n",
        "            if d != None:\n",
        "                sims.append(d)\n",
        "    \n",
        "        #print(sims)\n",
        "        #print(max(sims))\n",
        "        if sims != []:        \n",
        "           max_sim = max(sims)\n",
        "           #print(max_sim)\n",
        "           simi.append(max_sim)\n",
        "             \n",
        "    if simi != []:\n",
        "        max_final = max(simi)\n",
        "        final.append(max_final)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4hWBteXYUf7",
        "outputId": "5ddf87dc-5771-4c73-a89b-b5369670ece2"
      },
      "source": [
        "similarity_index = numpy.mean(final)\n",
        "similarity_index = round(similarity_index , 2)\n",
        "print(\"Sentence 1: \",str1)\n",
        "print(\"Sentence 2: \",str2)\n",
        "print(\"Similarity index value : \", similarity_index)\n",
        "\n",
        "if similarity_index>0.8:\n",
        "    print(\"Similar\")\n",
        "elif similarity_index>=0.6:\n",
        "    print(\"Somewhat Similar\")\n",
        "else:\n",
        "    print(\"Not Similar\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence 1:  Debugging is the process that results in the removal of error. It is very important part of the successful testing.\n",
            "Sentence 2:  It is a process of removing errors.\n",
            "Similarity index value :  0.77\n",
            "Somewhat Similar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "iopqznbkZmxE",
        "outputId": "6ab4f7ce-d5c6-4000-fd73-50c4aded215d"
      },
      "source": [
        "!pip install scikit-learn~=0.22  \n",
        "!pip install gensim~=3.8  \n",
        "!pip install nltk~=3.4 "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn~=0.22 in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn~=0.22) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn~=0.22) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn~=0.22) (1.4.1)\n",
            "Collecting gensim~=3.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/4e/afe2315e08a38967f8a3036bbe7e38b428e9b7a90e823a83d0d49df1adf5/gensim-3.8.3-cp37-cp37m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2MB 123kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim~=3.8) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim~=3.8) (5.0.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim~=3.8) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim~=3.8) (1.19.5)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-3.8.3\n",
            "Collecting nltk~=3.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/37/9532ddd4b1bbb619333d5708aaad9bf1742f051a664c3c6fa6632a105fd8/nltk-3.6.2-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 26.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk~=3.4) (2019.12.20)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk~=3.4) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk~=3.4) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk~=3.4) (4.41.1)\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.6.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRoWvM_Dcgzl",
        "outputId": "b674dc36-6311-41c2-b0e4-aeb55c3786a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnyK0wxuY4GO",
        "outputId": "b77f46e3-10f8-4f9e-df18-c6cf3cb8b1c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cd /content/drive/MyDrive"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2lW91NfZH48"
      },
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from scipy.spatial.distance import pdist, squareform"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbQm02hXkNRF"
      },
      "source": [
        "filenames = glob.glob(\"1667_texts/*.txt\")\n",
        "# Parse those filenames to create a list of file keys (ID numbers)\n",
        "# You'll use these later on.\n",
        "filekeys = [f.split('/')[-1].split('.')[0] for f in filenames]\n",
        "\n",
        "# Create a CountVectorizer instance with the parameters you need\n",
        "vectorizer = CountVectorizer(input=\"filename\", max_features=1000, max_df=0.7)\n",
        "# Run the vectorizer on your list of filenames to create your wordcounts\n",
        "# Use the toarray() function so that SciPy will accept the results\n",
        "wordcounts = vectorizer.fit_transform(filenames).toarray()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ygh_jIvLaKpz"
      },
      "source": [
        "metadata = pd.read_csv(\"1667_metadata.csv\", index_col=\"TCP ID\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyavNFwYdRa6",
        "outputId": "b4b70769-c647-475e-8fc2-dcce006d10b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "euclidean_distances = pd.DataFrame(squareform(pdist(wordcounts)), index=filekeys, columns=filekeys)\n",
        "print(euclidean_distances)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          A23770    A25198    A25743  ...    A28171    A28209    A28989\n",
            "A23770  0.000000  5.567764  5.916080  ...  6.403124  7.745967  6.480741\n",
            "A25198  5.567764  0.000000  4.242641  ...  4.690416  6.708204  4.582576\n",
            "A25743  5.916080  4.242641  0.000000  ...  5.656854  7.280110  5.385165\n",
            "A26249  6.782330  5.196152  5.916080  ...  6.244998  7.615773  6.000000\n",
            "A26426  7.000000  5.656854  6.000000  ...  6.633250  7.416198  6.403124\n",
            "A26482  6.403124  4.898979  5.291503  ...  5.656854  6.244998  5.744563\n",
            "A28171  6.403124  4.690416  5.656854  ...  0.000000  6.082763  5.744563\n",
            "A28209  7.745967  6.708204  7.280110  ...  6.082763  0.000000  7.211103\n",
            "A28989  6.480741  4.582576  5.385165  ...  5.744563  7.211103  0.000000\n",
            "\n",
            "[9 rows x 9 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_xS6UrWdcAA",
        "outputId": "a4e6c934-a60b-4a82-d5c9-0e0d7abab028",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cosine_distances = pd.DataFrame(squareform(pdist(wordcounts, metric='cosine')), index=filekeys, columns=filekeys)\n",
        "print(cosine_distances)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          A23770    A25198    A25743  ...    A28171    A28209    A28989\n",
            "A23770  0.000000  0.931959  0.943386  ...  0.910913  0.935450  1.000000\n",
            "A25198  0.931959  0.000000  0.815100  ...  0.709043  0.894591  0.764298\n",
            "A25743  0.943386  0.815100  0.000000  ...  0.939477  1.000000  0.934628\n",
            "A26249  0.958333  0.795876  0.943386  ...  0.866369  0.903175  0.855662\n",
            "A26426  1.000000  0.933333  0.944530  ...  0.956356  0.841886  0.952860\n",
            "A26482  0.953171  0.847056  0.872743  ...  0.799750  0.637262  0.891852\n",
            "A28171  0.910913  0.709043  0.939477  ...  0.000000  0.585961  0.845697\n",
            "A28209  0.935450  0.894591  1.000000  ...  0.585961  0.000000  0.888197\n",
            "A28989  1.000000  0.764298  0.934628  ...  0.845697  0.888197  0.000000\n",
            "\n",
            "[9 rows x 9 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH5-TpturIQl"
      },
      "source": [
        "from re import sub\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "query_string = 'fruit and vegetables'\n",
        "documents = ['cars drive on the road', 'tomatoes are actually fruit']\n",
        "\n",
        "stopwords = ['the', 'and', 'are', 'a']\n",
        "\n",
        "# From: https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb\n",
        "def preprocess(doc):\n",
        "    # Tokenize, clean up input document string\n",
        "    doc = sub(r'<img[^<>]+(>|$)', \" image_token \", doc)\n",
        "    doc = sub(r'<[^<>]+(>|$)', \" \", doc)\n",
        "    doc = sub(r'\\[img_assist[^]]*?\\]', \" \", doc)\n",
        "    doc = sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \" url_token \", doc)\n",
        "    return [token for token in simple_preprocess(doc, min_len=0, max_len=float(\"inf\")) if token not in stopwords]\n",
        "\n",
        "# Preprocess the documents, including the query string\n",
        "corpus = [preprocess(document) for document in documents]\n",
        "query = preprocess(query_string)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfSi7P8YrM8a",
        "outputId": "0f28b34e-b1cd-4ee1-b341-0664800140d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        }
      },
      "source": [
        "!pip install gensim\n",
        "import gensim\n",
        "import gensim.downloader as api\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import TfidfModel\n",
        "from gensim.similarities import WordEmbeddingSimilarityIndex\n",
        "from gensim.similarities import SparseTermSimilarityMatrix\n",
        "from gensim.similarities import SoftCosineSimilarity\n",
        "\n",
        "# Load the model: this is a big file, can take a while to download and open\n",
        "glove = api.load(\"glove-wiki-gigaword-50\")    \n",
        "similarity_index = WordEmbeddingSimilarityIndex(glove)\n",
        "\n",
        "# Build the term dictionary, TF-idf model\n",
        "dictionary = Dictionary(corpus+[query])\n",
        "tfidf = TfidfModel(dictionary=dictionary)\n",
        "\n",
        "# Create the term similarity matrix.  \n",
        "similarity_matrix = SparseTermSimilarityMatrix(similarity_index, dictionary, tfidf)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.0.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-fd86656f6d3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpora\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mWordEmbeddingSimilarityIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarities\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparseTermSimilarityMatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarities\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSoftCosineSimilarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'WordEmbeddingSimilarityIndex'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}